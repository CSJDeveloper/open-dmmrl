data:
  !include ../base_data_setting.yml
  
environment:
  dotenv_path: .env

  seed: 42

model:

  trust_remote_code: true

  # Model
  # To be the reasoner model
  # Qwen/Qwen2.5-0.5B-Instruct
  # Qwen/Qwen2.5-1.5B-Instruct
  # Qwen/Qwen2.5-3B-Instruct
  # Qwen/Qwen2.5-7B-Instruct
  # Qwen/Qwen2.5-14B-Instruct
  # Qwen/Qwen2.5-32B-Instruct
  # Qwen/Qwen2.5-72B-Instruct
  # Ensure this model name is the same as the generator's
  model_name: &model_name Qwen/Qwen2.5-0.5B-Instruct
  model_type: &model_type qwen2.5-vl

  trust_remote_code: true
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2

  system_prompt: "You are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format: <think>\n...\n</think>\n<answer>\n...\n</answer>"



train:

  epoch: 1
  # 2e-5
  learning_rate: 2.0e-05
  warmup_ratio: 0.1
  per_device_train_batch_size: 2 #16
  gradient_accumulation_steps: 1 #4
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  max_steps: -1

  weight_decay: 0.01
  lr_scheduler: cosine


  GRPO:
   
    # vllm part
    use_vllm: False
    vllm_device: auto
    vllm_gpu_memory_utilization: 0.7
    ds3_gather_for_generation: True
    # temperature
    temperature : 0.9
    max_prompt_length: 512
    max_completion_length: 1024
    # G of the GRPO paper
    num_generations: 2 # 16
    # Coefficient of the KL divergence loss
    beta: 0.04

    reward_functions:
      - accuracy_reward
      - format_reward
    reward_weights:
      - 1.0
      - 1.0


  lora:
      # Rank parameter for LoRA. The smaller this value, the fewer parameters will be modified.
      # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
      r: 16
      # The lora target modules set here is motivated by 
      # https://medium.com/@simon.gsponer/a-comprehensive-guide-ii-finetuning-a-bert-llm-with-lora-and-make-it-pipeline-compatible-9508e3822907
      # Make the modules whose name end with the following terms to be trainable.
      target_modules:
        - q_proj
        - k_proj
        - v_proj
        - o_proj
        - gate_proj
        - up_proj
        - down_proj
      modules_to_save: 
        - plan_embeddings
        
      # Alpha parameter for LoRA. This value determines the strength of the applied LoRA.
      lora_alpha: 16
      # Supports any, but = 0 is optimized
      lora_dropout: 0
      # Supports any, but = "none" is optimized
      bias: none
      use_rslora: false


logging:

  log_steps: 1
  # path where to save, empty for no saving
  checkpoint_path: experiments/checkpoints
  result_path: experiments/results
  logging_path: experiments/loggings
  visualization_path: experiments/visualizations

  log_completions: true
  log_first_step: true
  log_strategy: steps

evaluation:

  do_eval: false
  per_device_eval_batch_size: 16
  



   





