# Make the settings the same as the VLM-R1

data:
  !include ../base_data_setting.yml
  
environment:
  dotenv_path: .env

  seed: 42

model:

  # Model
  # To be the reasoner model
  # Qwen/Qwen2.5-VL-3B-Instruct 
  # Qwen/Qwen2.5-VL-7B-Instruct 
  # Qwen/Qwen2.5-VL-72B-Instruct 
  # Ensure this model name is the same as the generator's
  model_name: &model_name Qwen/Qwen2.5-VL-3B-Instruct 
  model_type: &model_type qwen2.5-vl

  trust_remote_code: true
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
  min_pixels: 3136 # Defualt 256*28*28
  max_pixels: 12845056 # Defualt 1280*28*28
  
  system_prompt: "You are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format: <think>\n...\n</think>\n<answer>\n...\n</answer>"



train:

  epoch: 2
  # 2e-5
  learning_rate: 2.0e-05
  warmup_ratio: 0.1
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 1
  gradient_checkpointing: false
  gradient_checkpointing_kwargs:
    use_reentrant: false
  max_steps: -1

  bf16: true
  weight_decay: 0.0
  lr_scheduler: cosine


  GRPO:
    # vllm part
    use_vllm: true
    vllm_device: auto
    vllm_gpu_memory_utilization: 0.7
    ds3_gather_for_generation: true

    temperature: 0.9
    max_prompt_length: 512
    max_completion_length: 1024
    # G of the GRPO paper
    num_generations: 2
    # Coefficient of the KL divergence loss
    beta: 0.04

    reward_functions:
      - accuracy_reward
      - format_reward
    reward_weights:
      - 1.0
      - 1.0

  use_peft: false


logging:

  logging_steps: 1
  # path where to save, empty for no saving
  checkpoint_path: experiments/checkpoints
  result_path: experiments/results
  logging_path: experiments/loggings
  visualization_path: experiments/visualizations

  save_steps: 100
  log_completions: true
  logging_first_step: true
  logging_strategy: steps

evaluation:

  do_eval: false
  per_device_eval_batch_size: 16
  



   




