# Make the settings the same as the VLM-R1


data:
  !include ../base_data_setting.yml
  
environment:
  dotenv_path: .env

  seed: 42

model:

  # Model
  # To be the reasoner model
  # Qwen/Qwen2.5-VL-3B-Instruct 
  # Qwen/Qwen2.5-VL-7B-Instruct 
  # Qwen/Qwen2.5-VL-72B-Instruct 
  # Ensure this model name is the same as the generator's
  model_name: &model_name Qwen/Qwen2.5-VL-3B-Instruct 
  model_type: &model_type qwen2.5-vl

  trust_remote_code: true
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2
  min_pixels: 3136 # Defualt 256*28*28
  max_pixels: 12845056 # Defualt 1280*28*28
  
  system_prompt: "You are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format: <think>\n...\n</think>\n<answer>\n...\n</answer>"



train:

  epoch: 2
  # 2e-5
  learning_rate: 2.0e-05
  warmup_ratio: 0.1
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 2
  gradient_checkpointing: false
  gradient_checkpointing_kwargs:
    use_reentrant: false
  max_steps: -1

  bf16: true
  weight_decay: 0.0
  lr_scheduler: cosine

  GRPO:
    # vllm part
    use_vllm: true
    vllm_device: auto
    vllm_gpu_memory_utilization: 0.7

    temperature: 0.9
    max_prompt_length: 512
    max_completion_length: 1024
    # G of the GRPO paper
    num_generations: 8
    reward_functions:
      - accuracy_reward
      - format_reward
    reward_weights:
      - 1.0
      - 1.0

  use_peft: true
  lora:
      # Rank parameter for LoRA. The smaller this value, the fewer parameters will be modified.
      # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
      r: 16
      # The lora target modules set here is motivated by 
      # https://medium.com/@simon.gsponer/a-comprehensive-guide-ii-finetuning-a-bert-llm-with-lora-and-make-it-pipeline-compatible-9508e3822907
      # Make the modules whose name end with the following terms to be trainable.
      target_modules:
        - q_proj
        - k_proj
        - v_proj
        - o_proj
        
      # Alpha parameter for LoRA. This value determines the strength of the applied LoRA.
      lora_alpha: 16
      # Supports any, but = 0 is optimized
      lora_dropout: 0
      # Supports any, but = "none" is optimized
      bias: none
      use_rslora: false



logging:

  logging_steps: 1
  # path where to save, empty for no saving
  checkpoint_path: experiments/checkpoints
  result_path: experiments/results
  logging_path: experiments/loggings
  visualization_path: experiments/visualizations

  save_steps: 100
  log_completions: true
  logging_first_step: true
  logging_strategy: steps

evaluation:

  do_eval: false
  per_device_eval_batch_size: 16
  



   




