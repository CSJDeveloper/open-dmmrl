data:
  !include ../base_data_setting.yml
  
environment:
  dotenv_path: .env

  seed: 42

model:


  # Model
  # To be the reasoner model
  # Qwen/Qwen2.5-0.5B 
  # Qwen/Qwen2.5-1.5B 
  # Qwen/Qwen2.5-3B 
  # Qwen/Qwen2.5-7B 
  # Qwen/Qwen2.5-14B 
  # Qwen/Qwen2.5-32B 
  # Qwen/Qwen2.5-72B 
  # Ensure this model name is the same as the generator's
  model_name: &model_name Qwen/Qwen2.5-0.5B
  model_type: &model_type qwen2.5

  trust_remote_code: true
  model_revision: main
  torch_dtype: bfloat16
  attn_implementation: flash_attention_2

  system_prompt: "You are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format: <think>\n...\n</think>\n<answer>\n...\n</answer>"



train:

  epoch: 1
  # 2e-5
  learning_rate: 2.0e-05
  warmup_ratio: 0.1
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  max_steps: -1

  bf16: true
  weight_decay: 0.0
  lr_scheduler: cosine

  GRPO:
   
    # vllm part
    use_vllm: false
    vllm_device: auto
    vllm_gpu_memory_utilization: 0.7
    ds3_gather_for_generation: true
    
    # temperature
    temperature : 0.9
    max_prompt_length: 512
    max_completion_length: 1024
    # G of the GRPO paper
    num_generations: 2
    # Coefficient of the KL divergence loss
    beta: 0.04

    reward_functions:
      - accuracy_reward
      - format_reward
    reward_weights:
      - 1.0
      - 1.0


  use_peft: false
 

logging:

  logging_steps: 1
  # path where to save, empty for no saving
  checkpoint_path: experiments/checkpoints
  result_path: experiments/results
  logging_path: experiments/loggings
  visualization_path: experiments/visualizations

  # Set to default
  save_steps: 500
  log_completions: true
  logging_first_step: true
  logging_strategy: steps

evaluation:

  do_eval: false
  per_device_eval_batch_size: 16
  



   





